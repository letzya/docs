---
title: "Tyk Pump - Export Metrics to Persistent Datastore"
description: "How to configure Tyk Pump"
sidebarTitle: "Tyk Pump - Export Metrics to Persistent Datastore"
tags: ['Pump', 'CSV', 'Datadog', 'Elasticsearch', 'Logzio', 'Moesif', 'Splunk', 'Prometheus', 'Analytics Storage', 'Monitoring', 'Observability']
---

## Introduction

Traffic analytics are captured by the Gateway nodes and then temporarily stored in Redis.  The Tyk Pump is responsible for moving those analytics into a persistent data store, such as MongoDB, where the traffic can be analyzed.

## What is the Tyk Pump?

The Tyk Pump is our [open source](https://github.com/TykTechnologies/tyk-pump) analytics purger that moves the data generated by your Tyk nodes to any back-end. It is primarily used to display your analytics data in the Tyk Dashboard.

<Note>
The Tyk Pump is not currently configurable in our Tyk Cloud solution.
</Note>

### Tyk Pump Data Flow

Here's the architecture depending on your deployment model:

<Tabs>
<Tab title="Enterprise">

<img src="img/diagrams/tyk-selfmanaged-architecture-pump.png" alt="Tyk Enterprise Pump Architecture" />

</Tab>
<Tab title="Open Source">

<img src="img/diagrams/diagram_docs_pump-open-source@2x.png" alt="Tyk Open Source Pump Architecture" />

</Tab>
</Tabs>

Tyk-Pump is both extensible, and flexible- meaning it is possible to configure Tyk-Pump to send data to multiple different backends at the same time as depicted by Pump Backends (i) and (ii), MongoDB and Elasticsearch respectively in Figure 1. Tyk-Pump is scalable, both horizontally and vertically, as indicated by Instances "1", "2", and "n". Additionally, it is possible to apply filters that dictate WHAT analytics go WHERE, please see the [docs on sharded analytics configuration here](/api-management/tyk-pump#configuring-the-sharded-analytics).

| <img src="img/diagrams/diagram_docs_pump-configuration-multi-backend.png" alt="Configuration and Scaling of Tyk Pump" />  |
|--|
| Figure 1: An architecture diagram illustrating horizontal scaling of "n" Instances of Tyk-Pump each with two different backends. |

### Other Supported Backend Services

We list our [supported backends here](/api-management/tyk-pump#external-data-stores).

### Configuring your Tyk Pump

See [Tyk Pump Configuration](/api-management/tyk-pump#tyk-pump-configuration) for more details on setting up your Tyk Pump.

Tyk Pump can be horizontally scaled without causing duplicate data, please see the following Table for the supported permutations of Tyk Pump scaling. 

| Supported | Summary |
| -- | -- |
| ✅ | Single Pump Instance, Single Backend |
| ✅ | Single Pump Instance, Multiple Backend(s) |
| ✅ | Multiple Pump Instances, Same Backend(s)|
| ❌ | Multiple Pump Instances, Different Backend(s) |

## Getting Started

### Tyk Pump Configuration

The Tyk Pump is our Open Source analytics purger that moves the data generated by your Tyk nodes to any back-end. By moving the analytics into your supported database, it allows the Tyk Dashboard to display traffic analytics across all your Tyk Gateways.

#### Tyk Dashboard

##### MongoDB

The Tyk Dashboard uses the `mongo-pump-aggregate` collection to display analytics. This is different than the standard `mongo` pump plugin that will store individual analytic items into MongoDB. The aggregate functionality was built to be fast, as querying raw analytics is expensive in large data sets. See [Pump Dashboard Config](/api-management/tyk-pump#setup-dashboard-analytics) for more details.

##### SQL

<Note>
Tyk no longer supports SQLite as of Tyk 5.7.0. To avoid disruption, please transition to [PostgreSQL](/tyk-self-managed#postgresql), [MongoDB](/tyk-self-managed#mongodb), or one of the listed compatible alternatives.
</Note>

In v4.0 of the Tyk Dashboard, we added support for the following SQL platforms:
- PostgreSQL
- SQLite

Within your Dashboard configuration file (`tyk-analytics.conf`) there is now a `storage` section.

```{.shell}
{
  ...
  "storage": {
    "main":{},
    "analytics":{},
    "logs":{},
    "uptime": {}
  }
}
```
###### Field description

- `main` - Main storage (APIs, Policies, Users, User Groups, etc.)
- `analytics` - Analytics storage (used for display all the charts and for all analytics screens)
- `logs` - Logs storage (log browser page)
- `uptime` - uptime tests analytics data

###### Common settings

For every `storage` section, you must populate the following fields:
```{.shell}
{
...
  "storage": {
    ...
    "main": {
      "type": "postgres",
      "connection_string": "user=root password=admin database=tyk-demo-db host=tyk-db port=5432",
    }
  }
}
```
- `type` use this field to define your SQL platform (currently SQLite or PostgreSQL are supported)
- `connection_string` the specific connection settings for your platform

The pump needed for storing logs data in the database, is very similar to other pumps as well as the storage setting in your Tyk Dashboard config. It just requires the `sql` name and database specific configuration options.

####### SQL example

```{.shell}
"sql": {
  "name": "sql",
  "meta": {
    "type": "postgres",
    "connection_string": "user=laurentiughiur password=test123 database=tyk-demo-db host=127.0.0.1 port=5432"
  }
},
```

#### Capping analytics data

Tyk Gateways can generate a lot of analytics data. Be sure to read about [capping your Dashboard analytics](/api-management/tyk-pump#tyk-pump-capping-analytics-data-storage)

#### Omitting the configuration file

From Tyk Pump 1.5.1+, you can configure an environment variable to omit the configuration file with the `TYK_PMP_OMITCONFIGFILE` variable.
This is specially useful when using Docker, since by default, the Tyk Pump has a default configuration file with pre-loaded pumps.

#### Sharding analytics to different data sinks

In a multi-organization deployment, each organization, team, or environment might have their preferred analytics tooling. This capability allows the Tyk Pump to send analytics for different organizations or various APIs to different destinations. 
E.g.  Org A can send their analytics to MongoDB + DataDog 
while Org B can send their analytics to DataDog + expose the Prometheus metrics endpoint.

